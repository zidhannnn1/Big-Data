{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89f7684",
   "metadata": {},
   "source": [
    "# Hands-On Pertemuan 6: Data Processing dengan Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ce9d1",
   "metadata": {},
   "source": [
    "## Tujuan:\n",
    "- Memahami dan mempraktikkan data processing menggunakan Apache Spark.\n",
    "- Menggunakan Spark untuk operasi data yang efisien pada dataset besar.\n",
    "- Menerapkan teknik canggih dalam Spark untuk mengatasi kasus penggunaan nyata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c2f90",
   "metadata": {},
   "source": [
    "### 1. Pengenalan Spark DataFrames\n",
    "Spark DataFrame menyediakan struktur data yang optimal dengan operasi yang dioptimalkan untuk pemrosesan data besar, yang sangat mirip dengan DataFrame di Pandas atau di RDBMS.\n",
    "\n",
    "- **Tugas 1**: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "986d01c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----+\n",
      "|  Nama|        Pekerjaan|Gaji|\n",
      "+------+-----------------+----+\n",
      "|Zidhan|Software Engineer|9999|\n",
      "|Faizal|            Dosen|4600|\n",
      "|Ridwan|     Daboy Dancer|4100|\n",
      "| Restu|              CEO|7000|\n",
      "|   Rio|Software Engineer|9999|\n",
      "+------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh membuat DataFrame sederhana dan operasi dasar\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F \n",
    "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
    "\n",
    "data = [('Zidhan', 'Software Engineer', 9999),\n",
    "        ('Faizal', 'Dosen', 4600),\n",
    "        ('Ridwan', 'Daboy Dancer', 4100),\n",
    "        ('Restu', 'CEO', 7000),\n",
    "        ('Rio', 'Software Engineer', 9999)]\n",
    "columns = ['Nama', 'Pekerjaan', 'Gaji']\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca66b73",
   "metadata": {},
   "source": [
    "### 2. Transformasi Dasar dengan DataFrames\n",
    "Pemrosesan data meliputi transformasi seperti filtering, selections, dan aggregations. Spark menyediakan cara efisien untuk melaksanakan operasi ini.\n",
    "\n",
    "- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58232678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Nama|Gaji|\n",
      "+------+----+\n",
      "|Zidhan|9999|\n",
      "|Faizal|4600|\n",
      "|Ridwan|4100|\n",
      "| Restu|7000|\n",
      "|   Rio|9999|\n",
      "+------+----+\n",
      "\n",
      "+------+-----------------+----+\n",
      "|  Nama|        Pekerjaan|Gaji|\n",
      "+------+-----------------+----+\n",
      "|Zidhan|Software Engineer|9999|\n",
      "| Restu|              CEO|7000|\n",
      "|   Rio|Software Engineer|9999|\n",
      "+------+-----------------+----+\n",
      "\n",
      "+-----------------+---------+\n",
      "|        Pekerjaan|avg(Gaji)|\n",
      "+-----------------+---------+\n",
      "|            Dosen|   4600.0|\n",
      "|Software Engineer|   9999.0|\n",
      "|     Daboy Dancer|   4100.0|\n",
      "|              CEO|   7000.0|\n",
      "+-----------------+---------+\n",
      "\n",
      "+-----------------+-----------+-------+---------+\n",
      "|        Pekerjaan|AverageGaji|MaxGaji|TotalGaji|\n",
      "+-----------------+-----------+-------+---------+\n",
      "|            Dosen|     4600.0|   4600|     4600|\n",
      "|Software Engineer|     9999.0|   9999|    19998|\n",
      "|     Daboy Dancer|     4100.0|   4100|     4100|\n",
      "|              CEO|     7000.0|   7000|     7000|\n",
      "+-----------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh operasi transformasi DataFrame\n",
    "df.select('Nama', 'Gaji').show()\n",
    "df.filter(df['Gaji'] > 5000).show()\n",
    "df.groupBy('Pekerjaan').avg('Gaji').show()\n",
    "df.groupBy('Pekerjaan').agg(\n",
    "    F.mean('Gaji').alias('AverageGaji'),\n",
    "    F.max('Gaji').alias('MaxGaji'),\n",
    "    F.sum('Gaji').alias('TotalGaji')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89763d36",
   "metadata": {},
   "source": [
    "### 3. Bekerja dengan Tipe Data Kompleks\n",
    "Spark mendukung tipe data yang kompleks seperti maps, arrays, dan structs yang memungkinkan operasi yang lebih kompleks pada dataset yang kompleks.\n",
    "\n",
    "- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14701d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----+-----------------+-----------------+\n",
      "|  Nama|        Pekerjaan|Gaji|        BonusGaji|TotalCompensation|\n",
      "+------+-----------------+----+-----------------+-----------------+\n",
      "|Zidhan|Software Engineer|9999|999.9000000000001|          10998.9|\n",
      "|Faizal|            Dosen|4600|            460.0|           5060.0|\n",
      "|Ridwan|     Daboy Dancer|4100|            410.0|           4510.0|\n",
      "| Restu|              CEO|7000|            700.0|           7700.0|\n",
      "|   Rio|Software Engineer|9999|999.9000000000001|          10998.9|\n",
      "+------+-----------------+----+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_bonus = df.withColumn('BonusGaji', df['Gaji'] * 0.1)\n",
    "\n",
    "df_with_compensation = df_with_bonus.withColumn('TotalCompensation', df_with_bonus['Gaji'] + df_with_bonus['BonusGaji'])\n",
    "\n",
    "df_with_compensation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b58dd",
   "metadata": {},
   "source": [
    "### 4. Operasi Data Lanjutan\n",
    "Menggunakan Spark untuk operasi lanjutan seperti window functions, user-defined functions (UDFs), dan mengoptimalkan query.\n",
    "\n",
    "- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "035312eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----+----+------------+\n",
      "|  Nama|        Pekerjaan|Gaji|Rank|RunningTotal|\n",
      "+------+-----------------+----+----+------------+\n",
      "| Restu|              CEO|7000|   1|        7000|\n",
      "|Ridwan|     Daboy Dancer|4100|   1|        4100|\n",
      "|Faizal|            Dosen|4600|   1|        4600|\n",
      "|Zidhan|Software Engineer|9999|   1|        9999|\n",
      "|   Rio|Software Engineer|9999|   1|       19998|\n",
      "+------+-----------------+----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh menggunakan window functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "windowSpec = Window.partitionBy('Pekerjaan').orderBy('Gaji')\n",
    "df_with_rank = df.withColumn('Rank', F.rank().over(windowSpec))\n",
    "df_with_running_total = df_with_rank.withColumn('RunningTotal', F.sum('Gaji').over(windowSpec.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "\n",
    "# Menampilkan DataFrame akhir\n",
    "df_with_running_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a097ec",
   "metadata": {},
   "source": [
    "### 5. Kesimpulan dan Eksplorasi Lebih Lanjut\n",
    "Review apa yang telah dipelajari tentang pemrosesan data menggunakan Spark dan eksplorasi teknik lebih lanjut untuk mengoptimalkan pemrosesan data Anda.\n",
    "- **Tugas 5**: Buat ringkasan dari semua operasi yang telah dilakukan dan bagaimana teknik ini dapat diterapkan pada proyek data Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a334c6",
   "metadata": {},
   "source": [
    "\n",
    "Dalam praktikum ini, saya telah menjelajahi berbagai operasi dasar dan lanjutan menggunakan PySpark DataFrame, termasuk pembuatan DataFrame dari data karyawan, manipulasi kolom untuk menghitung bonus dan total kompensasi, serta penerapan window functions untuk menghitung ranking dan running totals. Teknik-teknik ini sangat berharga dalam analisis data, karena memungkinkan pengguna untuk melakukan pengelompokan dan agregasi yang kompleks dengan efisien. Dalam proyek nanti, seperti analisis karyawan atau laporan keuangan, teknik ini dapat diimplementasikan untuk mengidentifikasi pola, menghasilkan wawasan yang mendalam, dan mendukung pengambilan keputusan strategis. Dengan kemampuan untuk menangani data besar secara efektif, PySpark menjadi alat yang penting dalam memfasilitasi analisis data yang mendetail dan berbasis bukti dalam berbagai konteks bisnis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
